# src/pipeline_stages/categorization.py
import traceback
from celery import shared_task
from typing import List
import os
import sys
import time
from openai import RateLimitError

# --- í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê³„ì‚° ë° sys.path ìˆ˜ì • ---
_current_file_path = os.path.abspath(__file__)
_project_root = os.path.dirname(os.path.dirname(os.path.dirname(_current_file_path)))
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)

from src.config_loader.settings import SETTINGS
from src.pipeline_stages.content_analysis import content_analysis_task


def _call_llm_for_list_output(prompt_text: str, openai_client, model_name: str, expected_items: int) -> list:
    """LLMì„ í˜¸ì¶œí•˜ì—¬ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜ë°›ëŠ” í—¬í¼ í•¨ìˆ˜"""
    if not openai_client: return []
    
    max_retries = 3
    base_delay = 2  # ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    
    for attempt in range(max_retries):
        try:
            completion = openai_client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "You are an AI assistant that provides responses as comma-separated lists."},
                    {"role": "user", "content": prompt_text}
                ],
                temperature=0.2,
                max_tokens=200 # í‚¤ì›Œë“œ ì¶”ì¶œì— ì¶©ë¶„í•œ í† í°
            )
            raw_output = completion.choices[0].message.content or ""
            parsed_list = [item.strip() for item in raw_output.strip().split(',') if item.strip()]
            return parsed_list[:expected_items]
            
        except RateLimitError as e:
            if attempt < max_retries - 1:
                wait_time = base_delay * (2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                print(f"Rate limit reached, waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}")
                time.sleep(wait_time)
                continue
            else:
                print(f"Rate limit error after {max_retries} attempts: {e}")
                return []
                
        except Exception as e:
            print(f"Categorization LLM call error: {e}")
            if attempt < max_retries - 1:
                wait_time = base_delay * (2 ** attempt)
                print(f"LLM error, waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}")
                time.sleep(wait_time)
                continue
            else:
                return []
    
    return []

def extract_llm_internal_keywords(text_content: str, openai_client) -> list:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    if not openai_client or not text_content or len(text_content.strip()) < 20: return []
    
    # config.yamlì—ì„œ ìµœëŒ€ í‚¤ì›Œë“œ ê°œìˆ˜ ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 10 ì‚¬ìš©)
    num_keywords = SETTINGS.get("LLM_CATEGORIZATION", {}).get("MAX_INTERNAL_KEYWORDS", 10)
    keyword_model = SETTINGS.get("OPENAI_KEYWORD_MODEL", "gpt-4.1-nano")
    content_for_llm = text_content[:4000]

    system_prompt = "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì£¼ì œë¥¼ ì™„ë²½í•˜ê²Œ ê´€í†µí•˜ëŠ” ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤."
    user_prompt = (
        f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ê°€ì¥ ì˜ ëŒ€í‘œí•˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œë¥¼ 3ê°œì—ì„œ {num_keywords}ê°œ ì‚¬ì´ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. "
        f"ë°˜ë“œì‹œ ê¸°ì‚¬ ì „ì²´ì˜ í•µì‹¬ ì˜ë¯¸ë¥¼ ê´€í†µí•˜ëŠ” ë‹¨ì–´ë“¤ì´ì–´ì•¼ í•©ë‹ˆë‹¤. "
        f"ê° í‚¤ì›Œë“œëŠ” ëª…ì‚¬ ë˜ëŠ” ëª…ì‚¬êµ¬ í˜•íƒœì—¬ì•¼ í•˜ë©°, ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
        f"í…ìŠ¤íŠ¸:\n{content_for_llm}\n\n"
        f"ì£¼ìš” í‚¤ì›Œë“œ (3~{num_keywords}ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„):"
    )
    return _call_llm_for_list_output(user_prompt, openai_client, keyword_model, num_keywords)


@shared_task(bind=True, max_retries=2, default_retry_delay=180)
def categorization_task(self, article_data: dict):
    """
    Celery Task: ê¸°ì‚¬ ë‚´ìš©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ê¸°ëŠ¥ì€ ì œê±°ë¨)
    """
    task_id_log = f"(Task ID: {self.request.id})" if self.request.id else ""
    print(f"\nğŸ“° Categorization Task (Keywords Only): ì²˜ë¦¬ ì‹œì‘ {task_id_log} - {article_data.get('url', 'URL ì—†ìŒ')[:70]}...")
    current_stage_name_path = "stage3_categorization_celery"

    # í•„ìš”í•  ë•Œë§Œ ê°€ì ¸ì˜¤ê¸°
    import src.celery_app
    worker_resources = src.celery_app.worker_resources
    save_to_data_folder = src.celery_app.save_to_data_folder

    # DART ê³µì‹œëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ ì—†ì´ í†µê³¼
    if article_data.get("source") == "DART":
        print(f"  â„¹ï¸ Stage 3 (Categorization Task): DART ì¶œì²˜ì´ë¯€ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤. {task_id_log}")
        article_data["llm_internal_keywords"] = []
        article_data.setdefault("checked", {})["categorization"] = True
        article_data["checked"]["categorization_reason"] = "SKIPPED_IS_DART"
        save_to_data_folder(article_data, f"{current_stage_name_path}/passed_dart", "passed")
        content_analysis_task.delay(article_data)
        return {"article_id": article_data.get("ID"), "categorized": True, "reason": "DART_SKIPPED"}

    openai_client_for_nlp = worker_resources.get('openai_client')
    if not openai_client_for_nlp:
        print(f"âŒ Categorization Task WARNING: OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. {task_id_log}")
        article_data["llm_internal_keywords"] = []
        article_data.setdefault("checked", {})["categorization_reason"] = "NO_OPENAI_CLIENT"
        article_data["checked"]["categorization"] = False
        save_to_data_folder(article_data, f"{current_stage_name_path}/skipped_no_client", "skipped")
        content_analysis_task.delay(article_data)
        return {"article_id": article_data.get("ID"), "categorized": False}

    content_to_analyze = (article_data.get("content", "") or (article_data.get("title", "") + " " + article_data.get("summary", ""))).strip()
    categorized_successfully = False

    if not content_to_analyze:
        article_data["llm_internal_keywords"] = []
        article_data.setdefault("checked", {})["categorization_reason"] = "NO_TEXT_FOR_ANALYSIS"
    else:
        try:
            # --- í‚¤ì›Œë“œ ì¶”ì¶œë§Œ ìˆ˜í–‰ ---
            keywords = extract_llm_internal_keywords(content_to_analyze, openai_client_for_nlp)
            article_data["llm_internal_keywords"] = keywords
            categorized_successfully = bool(keywords)
        except Exception as e:
            print(f"Categorization Task LLM Error: {e} {task_id_log}")
            article_data.setdefault("checked", {})["categorization_reason"] = f"LLM_ERROR: {str(e)[:100]}"
            raise self.retry(exc=e)

    article_data.setdefault("checked", {})["categorization"] = categorized_successfully
    if categorized_successfully:
        print(f"  âœ… Stage 3 (Categorization Task): í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ. ê¸°ì‚¬: {article_data.get('url')} {task_id_log}")
        save_to_data_folder(article_data, f"{current_stage_name_path}/passed", "passed")
    else:
        reason = article_data.get("checked", {}).get("categorization_reason", "í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
        print(f"  âš ï¸ Stage 3 (Categorization Task): ì‹¤íŒ¨ ë˜ëŠ” ê±´ë„ˆëœ€. ì´ìœ : {reason}. ê¸°ì‚¬: {article_data.get('url')} {task_id_log}")
        save_to_data_folder(article_data, f"{current_stage_name_path}/skipped_or_failed", "skipped")

    # ë‹¤ìŒ ë‹¨ê³„ì¸ content_analysis_taskë¡œ ì „ë‹¬
    content_analysis_task.delay(article_data)
    
    return {"article_id": article_data.get("ID"), "categorized": categorized_successfully}
