# src/pipeline_stages/content_extraction.py
import os
import re
from datetime import datetime
from bs4 import BeautifulSoup
from newspaper import Article as NewspaperArticle, Config as NewspaperConfig, ArticleException
import nltk
import ssl
import requests
from requests.exceptions import SSLError as RequestsSSLError
from typing import Tuple, Optional, Dict
import sys
from celery import shared_task
import src.celery_app


_current_file_path = os.path.abspath(__file__)
_project_root = os.path.dirname(os.path.dirname(os.path.dirname(_current_file_path)))
print(f"DEBUG (content_extraction.py): í˜„ì¬ íŒŒì¼ ê²½ë¡œ: {_current_file_path}")
print(f"DEBUG (content_extraction.py): ê³„ì‚°ëœ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {_project_root}")

if _project_root not in sys.path:
    sys.path.insert(0, _project_root)
    print(f"DEBUG (content_extraction.py): sys.pathì— ì¶”ê°€ë¨: {_project_root}")
else:
    print(f"DEBUG (content_extraction.py): {_project_root}ëŠ” ì´ë¯¸ sys.pathì— ìˆìŠµë‹ˆë‹¤.")
from src.pipeline_stages.categorization import categorization_task
from src.pipeline_stages.content_analysis import content_analysis_task
from src.config_loader.settings import SETTINGS
REPLACEMENT_CHAR = SETTINGS.get("REPLACEMENT_CHAR", '\ufffd')

_nltk_punkt_initialized = False
def initialize_nltk_punkt_once():
    global _nltk_punkt_initialized
    if _nltk_punkt_initialized:
        return
    try:
        nltk.data.find('tokenizers/punkt')
        # print("ContentExtraction Task: 'punkt' í† í¬ë‚˜ì´ì € ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨.")
    except LookupError:
        print("ContentExtraction Task: 'punkt' í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì‹œë„...")
        try:
            try: _create_unverified_https_context = ssl._create_unverified_context
            except AttributeError: pass
            else: ssl._create_default_https_context = _create_unverified_https_context
            nltk.download('punkt', quiet=True, raise_on_error=True)
            nltk.data.find('tokenizers/punkt')
            print("ContentExtraction Task: 'punkt' ë‹¤ìš´ë¡œë“œ ë° í™•ì¸ ì™„ë£Œ.")
        except Exception as e_nltk:
            print(f"ContentExtraction Task Error: 'punkt' ìë™ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e_nltk}")
            raise RuntimeError(f"NLTK Punkt ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e_nltk}")
    except Exception as e_nltk_find:
         print(f"ContentExtraction Task Error: NLTK 'punkt' í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e_nltk_find}")
         raise RuntimeError(f"NLTK Punkt í™•ì¸ ì‹¤íŒ¨: {e_nltk_find}")
    _nltk_punkt_initialized = True


# --- Aigen_science/src/processor/processor.pyì˜ fetch_article_with_newspaper3k ë¡œì§ í†µí•© ---
def fetch_article_with_newspaper3k(url: str, language_code='ko') -> Optional[Dict]:
    if not url:
        return None
    config = NewspaperConfig()
    config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'
    config.request_timeout = 15
    config.memoize_articles = False
    config.fetch_images = False
    article_parser = NewspaperArticle(url, language=language_code, config=config)
    try:
        article_parser.download()
        if article_parser.download_state != 2: # SUCCESS
            return None
        article_parser.parse()
        title = article_parser.title
        text = article_parser.text
        publish_date_obj = article_parser.publish_date

        publish_date_str = None
        if isinstance(publish_date_obj, datetime):
            publish_date_str = publish_date_obj.isoformat()

        return {"title_extracted": title, "content_extracted": text, "publish_date_extracted": publish_date_str}
    except ArticleException as e:
        return None
    except Exception as e:
        return None

# --- Aigen_science/src/processor/processor.pyì˜ fetch_content_with_beautifulsoup ë¡œì§ í†µí•© ---
def fetch_content_with_beautifulsoup(url: str, is_naver_news: bool = False) -> Optional[str]:
    if not url: return None
    session = requests.Session()
    session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'})
    response = None
    try:
        response = session.get(url, timeout=10)
        response.raise_for_status()
    except RequestsSSLError:
        try:
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            response = session.get(url, timeout=10, verify=False)
            response.raise_for_status()
        except Exception:
            return None
    except Exception:
        return None

    if response is None: return None

    html_content_bytes = response.content
    decoded_html_text = None
    potential_encodings = ['utf-8']
    if response.encoding:
        potential_encodings.append(response.encoding.lower())
    if response.apparent_encoding:
        potential_encodings.append(response.apparent_encoding.lower())
    potential_encodings.extend(['euc-kr', 'cp949', 'iso-8859-1'])

    unique_encodings = []
    for enc in potential_encodings:
        if enc and enc not in unique_encodings:
            unique_encodings.append(enc)

    for enc in unique_encodings:
        try:
            decoded_html_text = html_content_bytes.decode(enc)
            if REPLACEMENT_CHAR not in decoded_html_text:
                break
            decoded_html_text = None
        except UnicodeDecodeError:
            decoded_html_text = None
        except Exception:
            decoded_html_text = None

    if decoded_html_text is None:
        decoded_html_text = html_content_bytes.decode('utf-8', errors='replace')


    try:
        soup = BeautifulSoup(decoded_html_text, 'html.parser')
        article_body = None
        if is_naver_news:
            naver_selectors = [
                {'tag': 'div', 'attrs': {'id': 'dic_area'}},
                {'tag': 'div', 'attrs': {'class': 'newsct_article _article_body'}},
                {'tag': 'article', 'attrs': {'id': 'dic_area'}},
                {'tag': 'section', 'attrs': {'class': 'article-body'}},]
            for selector in naver_selectors:
                article_body = soup.find(selector['tag'], **selector['attrs'])
                if article_body: break
        if not article_body:
            general_selectors = [
                {'tag': 'article', 'attrs': {}},
                {'tag': 'div', 'attrs': {'class': re.compile(r'article-content|article_body|post-content|entry-content|ë³¸ë¬¸|article_view|articleBody|content|view_content', re.I)}},
                {'tag': 'main', 'attrs': {}},
                {'tag': 'div', 'attrs': {'id': re.compile(r'article_body|content|articleContent|realContent|viewContent', re.I)}},]
            for selector in general_selectors:
                article_body = soup.find(selector['tag'], **selector['attrs'])
                if article_body: break

        text_content = ""
        if article_body:
            for tag_to_remove in article_body.find_all(['script', 'style', 'aside', 'nav', 'footer', 'form', 'iframe', 'noscript', 'figure', 'figcaption', 'header',
                                                       'div.link_news', 'div.reporter_area', 'div.NCOMMENT', 'div#comment',
                                                       lambda tag: tag.has_attr('class') and any(cls in tag['class'] for cls in ['ad', 'banner', 'popup', 'related', 'share', 'comment', 'social', 'advertisement', 'widget', 'kst_link', 'link_news', 'journalistcard_card_article']) or
                                                                   tag.has_attr('id') and any(id_val in tag['id'] for id_val in ['ad', 'banner', 'popup', 'related', 'share', 'comment', 'social', 'advertisement', 'widget', 'ifr_recomm']) ]):
                if hasattr(tag_to_remove, 'decompose'): tag_to_remove.decompose()

            if is_naver_news:
                div_texts = [div.get_text(separator='\n', strip=True) for div in article_body.find_all(['div', 'p', 'span'], recursive=False) if div.get_text(strip=True) and len(div.get_text(strip=True)) > 10]
                if div_texts: text_content = "\n".join(div_texts)
                else: text_content = article_body.get_text(separator='\n', strip=True)
            else:
                paragraphs = article_body.find_all('p')
                if paragraphs:
                    extracted_paragraphs = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True) and len(p.get_text(strip=True)) > 20]
                    if extracted_paragraphs: text_content = "\n".join(extracted_paragraphs)
                    else: text_content = article_body.get_text(separator=' ', strip=True)
                else: text_content = article_body.get_text(separator=' ', strip=True)
        else:
            for tag_to_remove_global in soup.find_all(['script', 'style', 'aside', 'nav', 'footer', 'header', 'form', 'iframe', 'noscript',
                                                    lambda tag: tag.has_attr('class') and any(cls in tag['class'] for cls in ['ad', 'banner', 'popup', 'related', 'share', 'comment', 'social', 'advertisement', 'widget']) or
                                                                tag.has_attr('id') and any(id_val in tag['id'] for id_val in ['ad', 'banner', 'popup', 'related', 'share', 'comment', 'social', 'advertisement', 'widget'])]):
                if hasattr(tag_to_remove_global, 'decompose'): tag_to_remove_global.decompose()
            text_content = soup.get_text(separator=' ', strip=True)

        if text_content and REPLACEMENT_CHAR in text_content:
            pass
        return text_content
    except Exception as e_parsing:
        return None

# --- Aigen_science/src/processor/processor.pyì˜ get_html_for_llm ë¡œì§ í†µí•© ---
def get_html_for_llm(url: str) -> Optional[str]:
    if not url: return None
    try:
        headers = {'User-Agent': 'Googlebot/2.1 (+http://www.google.com/bot.html)'}
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()

        html_content_bytes = response.content
        decoded_html = None
        try:
            decoded_html = html_content_bytes.decode('utf-8')
            if REPLACEMENT_CHAR in decoded_html: decoded_html = None
        except UnicodeDecodeError:
            pass

        if not decoded_html and response.apparent_encoding:
            try:
                decoded_html = html_content_bytes.decode(response.apparent_encoding, errors='replace')
            except UnicodeDecodeError:
                pass

        if not decoded_html:
             decoded_html = html_content_bytes.decode('utf-8', errors='replace')

        return decoded_html
    except requests.exceptions.RequestException as e:
        return None
    except Exception as e_general:
        return None

# --- Aigen_science/src/processor/processor.pyì˜ try_llm_content_extraction_from_html ë¡œì§ í†µí•© ---
def try_llm_content_extraction_from_html(url: str, html_content: str, openai_client) -> Optional[str]:
    if not openai_client:
        print("ContentExtraction (LLM Extract): OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì œê³µë˜ì§€ ì•Šì•„ LLM ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    if not html_content:
        return None

    LLM_EXTRACTION_MODEL = SETTINGS.get("OPENAI_LLM_EXTRACTION_MODEL", "gpt-4.1-nano")
    html_snippet_for_llm = html_content[:80000]
    prompt = f"""
    ë‹¹ì‹ ì€ HTML ë¬¸ì„œì—ì„œ íŠ¹ì • ë‚´ìš©ì„ **ê·¸ëŒ€ë¡œ, ë‹¨ í•œ ê¸€ìë„ ë¹ ì§ì—†ì´, ì–´ë– í•œ ìš”ì•½ì´ë‚˜ ìˆ˜ì •, ì¬êµ¬ì„±ë„ í•˜ì§€ ì•Šê³ ** ì¶”ì¶œí•˜ëŠ” ë§¤ìš° ì •ë°€í•œ ë¡œë´‡ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ HTMLì—ì„œ ì˜¤ì§ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë³¸ë¬¸ ì „ì²´ë¥¼ ì‹œì‘ë¶€í„° ëê¹Œì§€ ë¬¸ì ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    **ì ˆëŒ€ ì¤€ìˆ˜ ê·œì¹™:**
    1.  **ìš”ì•½ ê¸ˆì§€**: ì ˆëŒ€ë¡œ ë‚´ìš©ì„ ìš”ì•½í•˜ê±°ë‚˜ ì§§ê²Œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
    2.  **ë‚´ìš© ë³€ê²½ ê¸ˆì§€**: ë‹¨ì–´ í•˜ë‚˜, ë¬¸ì¥ í•˜ë‚˜ë„ ì„ì˜ë¡œ ë³€ê²½í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”. ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
    3.  **ìƒëµ ê¸ˆì§€**: ê¸°ì‚¬ ë³¸ë¬¸ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ë¬¸ì¥ê³¼ ë¬¸ë‹¨ì„ ì‹œì‘ë¶€í„° ëê¹Œì§€ ì „ë¶€ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ê°„ì— ë‚´ìš©ì„ ìë¥´ê±°ë‚˜ ìƒëµí•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
    4.  **ë¶€ê°€ ìš”ì†Œ ì™„ë²½ ì œê±°**: ê´‘ê³ , ë©”ë‰´, ì‚¬ì´ë“œë°”, ê´€ë ¨ ê¸°ì‚¬ ë§í¬ ëª©ë¡, ëŒ“ê¸€, SNS ê³µìœ  ë²„íŠ¼, ì €ì‘ê¶Œ ê³ ì§€ ë“± ê¸°ì‚¬ ë³¸ë¬¸ì´ ì•„ë‹Œ ëª¨ë“  ê²ƒì€ ì² ì €íˆ ì œê±°í•´ì£¼ì„¸ìš”.
    5.  **í˜•ì‹ ìœ ì§€**: ì›ë³¸ ê¸°ì‚¬ì˜ ë¬¸ë‹¨ êµ¬ë¶„(ì¤„ë°”ê¿ˆ)ì„ ìµœëŒ€í•œ ë”°ë¼ì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
    6.  **ì¶œì²˜ ì •ë³´ í¬í•¨ (ì„ íƒ ì‚¬í•­, í•„ìš”í•œ ê²½ìš°)**: ë§Œì•½ ê¸°ì‚¬ ë³¸ë¬¸ ë°”ë¡œ ë’¤ì— 'ì¶œì²˜: [ì–¸ë¡ ì‚¬ëª…]' ë˜ëŠ” ìœ ì‚¬í•œ í˜•íƒœì˜ ì¶œì²˜ ì •ë³´ê°€ ëª…í™•íˆ ìˆë‹¤ë©´, í•´ë‹¹ ì¤„ê¹Œì§€ í¬í•¨í•˜ì—¬ ì¶”ì¶œí•´ì£¼ì„¸ìš”. (ë§Œì•½ ì´ ì§€ì‹œê°€ ë°©í•´ê°€ ëœë‹¤ë©´ ë¬´ì‹œí•´ë„ ì¢‹ìŠµë‹ˆë‹¤.)
    ì•„ë˜ ì œê³µëœ HTML ë‚´ìš©ì—ì„œ ìœ„ ê·œì¹™ë“¤ì„ ì² ì €íˆ ì§€ì¼œ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì „ì²´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
    HTML ë‚´ìš©:
    ```html
    {html_snippet_for_llm}
    ```
    ì •ì œëœ ê¸°ì‚¬ ë³¸ë¬¸:
    """
    try:
        completion = openai_client.chat.completions.create(
            model=LLM_EXTRACTION_MODEL,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ HTMLì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì›ë¬¸ ê·¸ëŒ€ë¡œ, ì–´ë– í•œ ë³€ê²½ì´ë‚˜ ìš”ì•½ ì—†ì´ ì¶”ì¶œí•˜ëŠ” ì´ˆì •ë°€ AI ë„êµ¬ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=3500
        )
        content = completion.choices[0].message.content.strip()
        if "ë³¸ë¬¸ ì¶”ì¶œ ë¶ˆê°€" in content or len(content) < 50:
            return None
        return content
    except Exception as e:
        return None

# --- Aigen_science/src/processor/processor.pyì˜ final_text_clean ë¡œì§ í†µí•© ---
def final_text_clean(text: str) -> str:
    if not isinstance(text, str) or not text.strip(): return ""
    text = text.replace(REPLACEMENT_CHAR, " ")
    soup = BeautifulSoup(text, "html.parser")
    text_no_html = soup.get_text(separator=' ', strip=True)
    cleaned_text = re.sub(r"[^a-zA-Z0-9ã„±-ã…ê°€-í£.?!,%\s]", " ", text_no_html)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    cleaned_text = cleaned_text.lower()
    return cleaned_text

LLM_SUMMARY_MODEL = SETTINGS.get("OPENAI_LLM_SUMMARY_MODEL", "gpt-4.1-nano") # config.yamlì— ì¶”ê°€ í•„ìš”

# --- ìƒˆë¡œìš´ LLM ìš”ì•½ í•¨ìˆ˜ ì¶”ê°€ ---
def _summarize_with_llm(text_content: str, openai_client) -> str:
    if not openai_client or not text_content or len(text_content.strip()) < 100: # ìµœì†Œ 100ì ì´ìƒ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ìš”ì•½ ì‹œë„
        return "" # ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

    content_for_llm = text_content[:8000] # ìš”ì•½í•  í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë³¸ë¬¸ë³´ë‹¤ ì§§ê²Œ)

    system_prompt = "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ 5ì¤„ ì´í•˜ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ AIì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨í•˜ê³ , ë¶ˆí•„ìš”í•œ ì„œë¡ ì´ë‚˜ ê²°ë¡  ì—†ì´ ë°”ë¡œ ë³¸ë¬¸ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”. ìš”ì•½ì€ í•œêµ­ì–´ë¡œ ì œê³µ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."
    user_prompt = (
        f"ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ 6ì¤„ ì´í•˜ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n"
        f"ë³¸ë¬¸:\n{content_for_llm}\n\n"
        f"ìš”ì•½:"
    )

    try:
        completion = openai_client.chat.completions.create(
            model=LLM_SUMMARY_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.5, # ìš”ì•½ì˜ ì°½ì˜ì„± ì¡°ì ˆ
            max_tokens=300 # 6ì¤„ ìš”ì•½ì— ì ì ˆí•œ í† í° ìˆ˜ (ëŒ€ëµ 1ì¤„ 50í† í° * 6ì¤„)
        )
        summary = completion.choices[0].message.content.strip()
        print(f"  ContentExtraction (LLM Summary): LLMìœ¼ë¡œ ìš”ì•½ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(summary)}).")
        return summary
    except Exception as e:
        print(f"  ContentExtraction (LLM Summary): LLM ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    
# --- content_extraction_task ë‚´ì—ì„œ LLM ìš”ì•½ í•¨ìˆ˜ í˜¸ì¶œ ---
@shared_task(bind=True, max_retries=2, default_retry_delay=120)
def content_extraction_task(self, article_data: dict):
    task_id_log = f"(Task ID: {self.request.id})" if self.request.id else ""
    print(f"\nğŸ“° Content Extraction Task: ì²˜ë¦¬ ì‹œì‘ {task_id_log} - {article_data.get('url', 'URL ì—†ìŒ')[:70]}...")
    current_stage_name_path = "stage2_content_extraction_celery"

    # í•„ìš”í•  ë•Œë§Œ ê°€ì ¸ì˜¤ê¸°
    worker_resources = src.celery_app.worker_resources
    save_to_data_folder = src.celery_app.save_to_data_folder

    try:
        initialize_nltk_punkt_once()
    except RuntimeError as e:
        print(f"âŒ Content Extraction Task CRITICAL: NLTK ì´ˆê¸°í™” ì‹¤íŒ¨: {e} {task_id_log}")
        save_to_data_folder(article_data, f"{current_stage_name_path}/error_nltk_init", "error")
        raise self.retry(exc=e, countdown=300, max_retries=1)

    openai_client_for_extraction = worker_resources.get('openai_client')

    article_url = article_data.get("url")
    original_title = article_data.get("title", "ì œëª© ì—†ìŒ (ì›ë³¸)")
    article_summary_original = article_data.get("summary", "")
    article_source_tag = article_data.get("source", "").upper()

    content_candidate = None
    content_source_log = "pending_extraction"
    extracted_title_candidate = None
    extracted_publish_date_candidate_str = None

    is_naver_news_link = bool(article_url and ("n.news.naver.com/mnews/article" in article_url or "sports.naver.com" in article_url)) # ìˆ˜ì •ëœ ì½”ë“œ

    # ì½˜í…ì¸  ì¶”ì¶œ ì „ëµ (ì›ë³¸ í•¨ìˆ˜ì˜ ë¡œì§ì„ ìµœëŒ€í•œ ë”°ë¦„)
    if is_naver_news_link:
        content_candidate = fetch_content_with_beautifulsoup(article_url, is_naver_news=True)
        content_source_log = "beautifulsoup_naver"
        if not (content_candidate and len(content_candidate.strip()) > 50):
            extracted_np_data = fetch_article_with_newspaper3k(article_url)
            if extracted_np_data and extracted_np_data.get("content_extracted") and \
               len(extracted_np_data["content_extracted"].strip()) > 50:
                content_candidate = extracted_np_data["content_extracted"]
                content_source_log = "newspaper3k_after_bs_fail_naver"
                if extracted_np_data.get("title_extracted"): extracted_title_candidate = extracted_np_data["title_extracted"]
                if extracted_np_data.get("publish_date_extracted"): extracted_publish_date_candidate_str = extracted_np_data["publish_date_extracted"]
            else:
                content_candidate = None
    else: # ì¼ë°˜ URL
        extracted_np_data = fetch_article_with_newspaper3k(article_url)
        if extracted_np_data and extracted_np_data.get("content_extracted") and \
           len(extracted_np_data["content_extracted"].strip()) > 50:
            content_candidate = extracted_np_data["content_extracted"]
            content_source_log = "newspaper3k_general"
            if extracted_np_data.get("title_extracted"): extracted_title_candidate = extracted_np_data["title_extracted"]
            if extracted_np_data.get("publish_date_extracted"): extracted_publish_date_candidate_str = extracted_np_data["publish_date_extracted"]
        else:
            content_candidate = fetch_content_with_beautifulsoup(article_url, is_naver_news=False)
            content_source_log = "beautifulsoup_general_fallback"
            if not (content_candidate and len(content_candidate.strip()) > 50):
                content_candidate = None

    if not (content_candidate and len(content_candidate.strip()) > 50) and article_source_tag != "DART":
        html_for_llm = get_html_for_llm(article_url)
        if html_for_llm and openai_client_for_extraction:
            llm_extracted_content = try_llm_content_extraction_from_html(article_url, html_for_llm, openai_client_for_extraction)
            if llm_extracted_content and len(llm_extracted_content.strip()) > 50:
                content_candidate = llm_extracted_content
                content_source_log = "llm_extraction_from_html"
        elif not openai_client_for_extraction:
             content_source_log += "_llm_skipped_no_client"
        else:
            content_source_log += "_llm_skipped_no_html"

    final_content_text = content_candidate if content_candidate is not None else ""

    # ì œëª© ì—…ë°ì´íŠ¸
    if extracted_title_candidate:
        article_data["title"] = final_text_clean(extracted_title_candidate)
    else:
        article_data["title"] = final_text_clean(original_title)

    # ë°œí–‰ì¼ ì—…ë°ì´íŠ¸ (Newspaper3kê°€ ë” ì •í™•í•œ ê°’ì„ ì œê³µí–ˆì„ ê²½ìš°)
    if extracted_publish_date_candidate_str:
        article_data["published_at"] = extracted_publish_date_candidate_str

    # final_content_textê°€ í™•ì •ëœ í›„ ìš”ì•½ ìƒì„±
    article_data["content"] = final_text_clean(final_content_text)

    # **ì—¬ê¸°ì„œ LLM ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.**
    # ì¶”ì¶œëœ ë³¸ë¬¸ì´ ìˆê³  OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ìˆë‹¤ë©´ ìš”ì•½ ìƒì„±
    if article_data["content"] and openai_client_for_extraction:
        llm_generated_summary = _summarize_with_llm(article_data["content"], openai_client_for_extraction)
        if llm_generated_summary:
            article_data["summary"] = llm_generated_summary # LLM ìš”ì•½ìœ¼ë¡œ summary í•„ë“œ ì—…ë°ì´íŠ¸
            print(f"  âœ… Stage 2 (Content Extraction Task): LLM ìš”ì•½ ì™„ë£Œ.")
        else:
            # LLM ìš”ì•½ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ìš”ì•½ (collectorì—ì„œ ì˜¨ ê²ƒ)ì„ ì •ì œí•˜ì—¬ ì‚¬ìš©
            article_data["summary"] = final_text_clean(article_summary_original)
            print(f"  âš ï¸ Stage 2 (Content Extraction Task): LLM ìš”ì•½ ì‹¤íŒ¨. ê¸°ì¡´ ìš”ì•½ ì •ì œë³¸ ì‚¬ìš©.")
    else:
        # ë³¸ë¬¸ì´ ì—†ê±°ë‚˜ LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ë‹¤ë©´ ê¸°ì¡´ ìš”ì•½ ì •ì œë³¸ ì‚¬ìš©
        article_data["summary"] = final_text_clean(article_summary_original)
        print(f"  â„¹ï¸ Stage 2 (Content Extraction Task): LLM ìš”ì•½ ê±´ë„ˆëœ€ (ë³¸ë¬¸ ì—†ìŒ/í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ). ê¸°ì¡´ ìš”ì•½ ì •ì œë³¸ ì‚¬ìš©.")


    article_data.setdefault("checked", {})
    article_data["checked"]["content_source_log"] = content_source_log

    extracted_successfully = False
    # ì„±ê³µ ì—¬ë¶€ íŒë‹¨ (ì›ë³¸ ë¡œì§ ìœ ì§€)
    if article_source_tag == "DART":
        if article_data.get("title") or article_data.get("summary"):
             extracted_successfully = True
        else:
            article_data["checked"]["content_extraction_reason"] = "DART_CONTENT_MISSING"
    elif len(article_data["content"].strip()) > 50:
        extracted_successfully = True
    else:
        article_data["checked"]["content_extraction_reason"] = "INSUFFICIENT_CONTENT_LENGTH"


    article_data["checked"]["content_extraction"] = extracted_successfully
    if not extracted_successfully:
        reason = article_data.get("checked", {}).get("content_extraction_reason", "ì½˜í…ì¸  ë¶€ì¡±")
        print(f"  â¡ï¸ Stage 2 (Content Extraction Task): ì‹¤íŒ¨. ì´ìœ : {reason}. ê¸°ì‚¬: {article_url} {task_id_log}")
        save_to_data_folder(article_data, f"{current_stage_name_path}/failed", "failed")
        # ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨ (ë‹¤ìŒ íƒœìŠ¤í¬ í˜¸ì¶œ ì•ˆ í•¨)
    else:
        print(f"  âœ… Stage 2 (Content Extraction Task): ì„±ê³µ. ê¸°ì‚¬: {article_url} {task_id_log}")
        save_to_data_folder(article_data, f"{current_stage_name_path}/passed", "passed")
        # ë‹¤ìŒ íƒœìŠ¤í¬ ê²°ì • (Categorization í™œì„±í™” ì—¬ë¶€ì— ë”°ë¼)
        if SETTINGS.get("LLM_CATEGORIZATION", {}).get("ENABLED", False):
            categorization_task.delay(article_data)
        else:
            print(f"  â„¹ï¸ Stage 3 (Categorization Task): ë¹„í™œì„±í™”ë¨. Content Analysis Taskë¡œ ì§„í–‰. {task_id_log}")
            article_data["checked"]["categorization"] = "skipped_disabled"
            content_analysis_task.delay(article_data)

    return {"article_id": article_data.get("ID"), "extracted": extracted_successfully}
