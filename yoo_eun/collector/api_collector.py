import requests
import html # HTML ì—”í‹°í‹° ë””ì½”ë”©ì„ ìœ„í•´ ì„í¬íŠ¸
import os # keys, sources íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì„í¬íŠ¸
from bs4 import BeautifulSoup # HTML íƒœê·¸ ì œê±°ë¥¼ ìœ„í•´ ì„í¬íŠ¸ (BeautifulSoup ì„¤ì¹˜ í•„ìš”: pip install beautifulsoup4)
import json # ë””ë²„ê¹… ì‹œ JSON ì¶œë ¥ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



def load_naver_keys(filepath="naver.keys.txt"):
    """ naver.keys.txt íŒŒì¼ì—ì„œ ë„¤ì´ë²„ API í‚¤(client_id, client_secret)ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤. """
    naver_client_id = None
    naver_client_secret = None
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # collector í´ë”ë¥¼ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    keys_filepath = os.path.join(base_dir, filepath) # naver.keys.txt íŒŒì¼ ê²½ë¡œ

    try:
        if not os.path.exists(keys_filepath):
            print(f"âŒ ì˜¤ë¥˜: Naver keys íŒŒì¼ '{keys_filepath}' ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None

        with open(keys_filepath, "r", encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and '=' in line and not line.startswith('#'):
                    key, value = line.split('=', 1)
                    if key.strip() == "client_id":
                        naver_client_id = value.strip()
                    elif key.strip() == "client_secret":
                        naver_client_secret = value.strip()

        if not naver_client_id or not naver_client_secret:
            print(f"âŒ ì˜¤ë¥˜: '{filepath}' íŒŒì¼ì—ì„œ client_id ë˜ëŠ” client_secret ê°’ì„ ì½ì–´ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
            return None, None

        print(f"âœ… ë„¤ì´ë²„ API í‚¤ ë¡œë“œ ì„±ê³µ")

        return naver_client_id, naver_client_secret

    except Exception as e:
        print(f"âŒ Naver keys íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return None, None



# --- HTML íƒœê·¸ ì œê±° í•¨ìˆ˜ ---
def strip_html_tags(text):
    """ BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
    if not text:
        return ""
    try:
        soup = BeautifulSoup(text, "html.parser")
        return soup.get_text(separator=' ', strip=True)
    except Exception as e:
        # print(f"âš ï¸ ì˜¤ë¥˜: HTML íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ | {e}") # ë””ë²„ê¹…ìš©
        return text


# --- API ì†ŒìŠ¤ íŒŒì¼ì—ì„œ URLì„ ì½ì–´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ ---
def collect_from_api_file(filepath="api_sources.txt"):
    """
    api_sources.txt íŒŒì¼ì— ë‚˜ì—´ëœ URLì—ì„œ API ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    URL íŒ¨í„´ì— ë”°ë¼ ë„¤ì´ë²„, thenewsapi.com, gnews.io ë“±ì„ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    thenewsapi.comê³¼ gnews.io í‚¤ëŠ” URLì— í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    """
    articles = []
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # í”„ë¡œì íŠ¸ ë£¨íŠ¸ í´ë”
    sources_filepath = os.path.join(base_dir, filepath) # api_sources.txt íŒŒì¼ ê²½ë¡œ

    # --- í•„ìš”í•œ API Key ë¡œë“œ (ìˆ˜ì§‘ ì‹œì‘ ì „ ë¯¸ë¦¬ ë¡œë“œ) ---
    print("âœ… API í‚¤ ë¡œë“œ ì‹œë„...")

    naver_client_id, naver_client_secret = load_naver_keys()
    naver_headers = None
    if naver_client_id and naver_client_secret:
        naver_headers = {
            "X-Naver-Client-Id": naver_client_id,
            "X-Naver-Client-Secret": naver_client_secret
        }
    else:
        print("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì—†ì–´ ë„¤ì´ë²„ API ìˆ˜ì§‘ ì‹œë„ ì‹œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")



    # api_sources.txt íŒŒì¼ì—ì„œ URL ëª©ë¡ ì½ê¸°
    urls = [] # URL ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    print(f"âœ… '{filepath}'ì—ì„œ API ì†ŒìŠ¤ URL ë¡œë“œ ì‹œë„...") # íŒŒì¼ ë¡œë“œ ì‹œë„ ë©”ì‹œì§€
    try:
        with open(sources_filepath, "r", encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        print(f"âœ… '{filepath}'ì—ì„œ {len(urls)}ê°œì˜ API ì†ŒìŠ¤ URL ë¡œë“œ ì™„ë£Œ") # ë¡œë“œëœ URL ìˆ˜ í™•ì¸
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: API ì†ŒìŠ¤ íŒŒì¼ '{sources_filepath}' ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return articles # íŒŒì¼ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    except Exception as e:
        print(f"âŒ API ì†ŒìŠ¤ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {sources_filepath} | {e}")
        return articles # ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜


    # --- URL ëª©ë¡ ìˆœíšŒí•˜ë©° ìˆ˜ì§‘ ì‹œì‘ ---
    print("\nğŸŒ API ìˆ˜ì§‘ ì‹œì‘...")
    total_items_collected = 0 # ì „ì²´ ìˆ˜ì§‘ëœ í•­ëª© ê°œìˆ˜ ì¹´ìš´í„°

    for url in urls:
        if not url.startswith('http'): # ìœ íš¨í•œ URL í˜•ì‹ì¸ì§€ ê°„ë‹¨íˆ í™•ì¸
            print(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ URL í˜•ì‹ ê±´ë„ˆëœ€: {url}")
            continue

        # --- URL íŒ¨í„´ì— ë”°ë¼ API êµ¬ë¶„ ë° ì²˜ë¦¬ ---

        # ê¸°ë³¸ ì‘ë‹µ ë°ì´í„° ë³€ìˆ˜ ì´ˆê¸°í™”
        data = None
        response = None # ì‘ë‹µ ê°ì²´ ì´ˆê¸°í™”

        try:
            # 1. URLì´ ë„¤ì´ë²„ ë‰´ìŠ¤ APIì¸ ê²½ìš°
            if "openapi.naver.com/v1/search/news.json" in url:
                if not naver_headers: # ë„¤ì´ë²„ í‚¤ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                    print(f"âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì—†ì–´ '{url}' ìˆ˜ì§‘ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ API ìˆ˜ì§‘: {url}")
                # ë„¤ì´ë²„ API í˜¸ì¶œ ì‹œ headers í•„ìš”
                response = requests.get(url, headers=naver_headers, timeout=10)

            # 2. URLì´ thenewsapi.comì¸ ê²½ìš° (í‚¤ëŠ” URLì— í¬í•¨ ê°€ì •)
            elif "thenewsapi.com/v1/news/headlines" in url or "thenewsapi.com/v1/news/top" in url:
                print(f"ğŸ” thenewsapi.com API ìˆ˜ì§‘: {url}")
                response = requests.get(url, timeout=10)

            # 3. URLì´ gnews.ioì¸ ê²½ìš° (í‚¤ëŠ” URLì— í¬í•¨ ê°€ì •)
            elif "gnews.io/api/v4" in url:

                print(f"ğŸ” gnews.io API ìˆ˜ì§‘: {url}")
                # URLì— í‚¤ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  URLë§Œ ê°€ì§€ê³  GET ìš”ì²­
                response = requests.get(url, timeout=10)

            # 4. ìœ„ì— ì •ì˜ëœ ì–´ë–¤ API íŒ¨í„´ê³¼ë„ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” URLì¸ ê²½ìš°
            else:
                # api_sources.txtì— ë‚˜ì—´ë˜ì—ˆì§€ë§Œ ì•Œë ¤ì§„ íŒ¨í„´(ë„¤ì´ë²„, thenewsapi.com, gnews.io)ì´ ì•„ë‹Œ ê²½ìš°
                print(f"â“ api_sources.txtì˜ ì•Œ ìˆ˜ ì—†ëŠ” URL í˜•ì‹ ê±´ë„ˆê¹€: {url}")
                continue # ì•Œ ìˆ˜ ì—†ëŠ” URLì€ ì—¬ê¸°ì„œ ê±´ë„ˆëœë‹ˆë‹¤.


            # --- API í˜¸ì¶œ ì„±ê³µ ë° ì‘ë‹µ ì²˜ë¦¬ (ìœ„ì—ì„œ response ê°ì²´ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ëœ ê²½ìš°) ---
            if response is not None:
                # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
                response.raise_for_status()

                # ì‘ë‹µì„ JSON í˜•ì‹ìœ¼ë¡œ íŒŒì‹±
                data = response.json()

                items = [] # ê¸°ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                source_type = "Unknown API" # ì†ŒìŠ¤ íƒ€ì… ì´ˆê¸°í™”

                # --- API ì‘ë‹µ êµ¬ì¡°ì— ë§ì¶° í•­ëª© ë¦¬ìŠ¤íŠ¸(items) ì°¾ê¸° ---
                if "openapi.naver.com" in url:
                    items = data.get("items", []) # ë„¤ì´ë²„ ì‘ë‹µì€ 'items' í‚¤ ì‚¬ìš©
                    source_type = "Naver News API"
                elif "thenewsapi.com" in url:
                    items = data.get("data", []) # thenewsapi.com ì‘ë‹µì€ 'data' í‚¤ ì‚¬ìš©
                    source_type = "thenewsapi.com"
                elif "gnews.io" in url:
                    items = data.get("articles", []) # gnews.io ì‘ë‹µì€ 'articles' í‚¤ ì‚¬ìš©
                    source_type = "gnews.io"
                # else: itemsëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€

                if not items:
                    print(f"âš ï¸ {source_type} ì‘ë‹µì— ì˜ˆìƒë˜ëŠ” ê¸°ì‚¬ ëª©ë¡ í‚¤ ë˜ëŠ” í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤: {url}")

                else:
                    print(f"âœ… {source_type} ì‘ë‹µ í•­ëª© ìˆ˜: {len(items)}")


                # --- í•­ëª© ë¦¬ìŠ¤íŠ¸(items) ìˆœíšŒí•˜ë©° ë°ì´í„° ì¶”ì¶œ ë° ì •ë¦¬ ---
                count_added_from_url = 0 # ì´ URLì—ì„œ ì‹¤ì œë¡œ ì¶”ê°€ëœ í•­ëª© ìˆ˜ ì¹´ìš´í„°
                for item in items:
                    if isinstance(item, dict): # í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ì§€ í™•ì¸

                        # ê° API ì‘ë‹µ êµ¬ì¡°ì— ë§ì¶° í•„ë“œ ì¶”ì¶œ
                        title = None
                        url_link = None
                        raw_summary = None
                        source_name = None
                        # published_at = None # ë°œí–‰ì¼ í•„ë“œ (í•„ìš”ì‹œ)

                        if source_type == "Naver News API":
                            title = item.get("title", "")
                            url_link = item.get("link") # ë„¤ì´ë²„ëŠ” 'link' í•„ë“œ
                            raw_summary = item.get("description", "") # ë„¤ì´ë²„ëŠ” 'description' í•„ë“œ
                            # ë„¤ì´ë²„ ì¶œì²˜ëŠ” titleì— í¬í•¨ë˜ê±°ë‚˜ ë³„ë„ source í•„ë“œ ì—†ìŒ (í˜¹ì€ originallinkì˜ ë„ë©”ì¸)
                            # ì—¬ê¸°ì„œëŠ” ì†ŒìŠ¤ íƒ€ì… ì´ë¦„ ì‚¬ìš©
                            source_name = source_type
                            # published_at = item.get('pubDate') # í•„ìš”ì‹œ ì¶”ê°€

                        elif source_type == "thenewsapi.com":
                            title = item.get("title", "")
                            url_link = item.get("url") # thenewsapi.comì€ 'url' í•„ë“œ
                            raw_summary = item.get("snippet", "") # thenewsapi.comì€ 'snippet' í•„ë“œ
                            source_name = item.get("source", source_type) # thenewsapi.comì€ 'source' í•„ë“œ (ë„ë©”ì¸)
                            # published_at = item.get('published_at') # í•„ìš”ì‹œ ì¶”ê°€

                        elif source_type == "gnews.io":
                            title = item.get("title", "")
                            url_link = item.get("url") # gnews.ioëŠ” 'url' í•„ë“œ
                            raw_summary = item.get("description", "") # gnews.ioëŠ” 'description' í•„ë“œ
                            # gnews.io ì¶œì²˜ëŠ” 'source' í•„ë“œì¸ë°, { 'name': '...' } ë”•ì…”ë„ˆë¦¬ í˜•íƒœ
                            source_info = item.get("source")
                            if isinstance(source_info, dict):
                                source_name = source_info.get("name", source_type)
                            else:
                                source_name = source_type # ë”•ì…”ë„ˆë¦¬ í˜•íƒœê°€ ì•„ë‹ˆë©´ ì†ŒìŠ¤ íƒ€ì… ì´ë¦„ ì‚¬ìš©
                            # published_at = item.get('publishedAt') # í•„ìš”ì‹œ ì¶”ê°€

                        # else: Unknown API -> í•„ë“œ ì¶”ì¶œ ì•ˆ ë¨ (ìœ„ì—ì„œ ê±¸ëŸ¬ì§€ì§€ë§Œ)

                        # --- ì¶”ì¶œëœ ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦ ---

                        # HTML ì—”í‹°í‹° ë””ì½”ë”© ë° íƒœê·¸ ì œê±° (summary/description/snippetì—ì„œ)
                        plain_text_summary = strip_html_tags(html.unescape(raw_summary)) # ì•ˆì „í•˜ê²Œ unescape í›„ íƒœê·¸ ì œê±°


                        # í•„ìˆ˜ í•„ë“œ(title, url, summary)ê°€ ìˆëŠ”ì§€ ìµœì¢… í™•ì¸
                        if not title or not url_link or not plain_text_summary:
                            # print(f"âš ï¸ ë¶ˆì™„ì „í•œ í•­ëª© (í•„ìˆ˜ í•„ë“œ ëˆ„ë½) ê±´ë„ˆëœ€: {item.get('title', 'ì œëª© ì—†ìŒ')[:50]}...") # ë””ë²„ê¹…ìš©
                            continue # ë¶ˆì™„ì „í•œ í•­ëª©ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ


                        # --- í•­ëª© ì¶”ê°€ ---
                        articles.append({
                            "title": title,
                            "url": url_link,
                            "summary": plain_text_summary, # HTML ì œê±°ëœ summary ì‚¬ìš©
                            "source": source_name, # ì¶”ì¶œ/ì •ë¦¬ëœ ì¶œì²˜ ì´ë¦„ ì‚¬ìš©
                            # "published_at": published_at # ë°œí–‰ì¼ (í•„ìš”ì‹œ ì¶”ê°€)
                        })
                        count_added_from_url += 1 # ì´ URLì—ì„œ ì‹¤ì œë¡œ ì¶”ê°€ëœ í•­ëª© ìˆ˜ ì¹´ìš´íŠ¸

                    else:
                        print(f"âš ï¸ ìŠ¤í‚µë¨: API ì‘ë‹µ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœê°€ ì•„ë‹˜ â†’ {item}") # í•­ëª© í˜•íƒœ ì˜¤ë¥˜ ë©”ì‹œì§€

                print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {url} ({count_added_from_url}ê°œ í•­ëª© ì¶”ê°€)") # ì´ URL ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½

            # --- ì˜¤ë¥˜ ì²˜ë¦¬ ---
        except requests.exceptions.RequestException as req_e:
                # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë°œìƒí•˜ëŠ” HTTP ì˜¤ë¥˜, ì—°ê²° ì˜¤ë¥˜ ë“± ì²˜ë¦¬
                print(f"âŒ API ìš”ì²­ ì˜¤ë¥˜: {url} | {req_e}")
                if req_e.response is not None:
                    print(f"    HTTP ìƒíƒœ ì½”ë“œ: {req_e.response.status_code}")
                    # ì‘ë‹µ ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ì¶œë ¥
                    try:
                        print(f"    ì‘ë‹µ ë‚´ìš©: {req_e.response.text[:500]}...") # ìµœëŒ€ 500ì ì¶œë ¥
                    except Exception:
                        print("    ì‘ë‹µ ë‚´ìš© í™•ì¸ ë¶ˆê°€")

        except ValueError as json_e:
                # response.json()ì—ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ (ì‘ë‹µ ë‚´ìš©ì´ JSON í˜•ì‹ì´ ì•„ë‹ ë•Œ) ì²˜ë¦¬
                print(f"âŒ API ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜: {url} | ì‘ë‹µ ë‚´ìš©ì´ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {json_e}")


        except Exception as e:
                # ìœ„ì— ì •ì˜ë˜ì§€ ì•Šì€ ëª¨ë“  ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ì²˜ë¦¬
                print(f"âŒ API ìˆ˜ì§‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {url} | {e}")


    # --- ì „ì²´ ìˆ˜ì§‘ ê³¼ì • ì™„ë£Œ ---
    print(f"\nâœ… API ìˆ˜ì§‘ ê³¼ì • ì™„ë£Œ. ì´ {len(articles)}ê°œ í•­ëª© ìˆ˜ì§‘.")
    return articles

# ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•  ë•Œ main í•¨ìˆ˜ í˜¸ì¶œ
if __name__ == "__main__":
    # ìˆ˜ì§‘ í•¨ìˆ˜ ì‹¤í–‰
    collected_data = collect_from_api_file()

    # ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í•„ìš”ì— ë”°ë¼ ì²˜ë¦¬ (ì˜ˆ: ì¶œë ¥, íŒŒì¼ ì €ì¥ ë“±)
    print("\n--- ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë°ì´í„° í™•ì¸ (ì¼ë¶€) ---")
    # ì²˜ìŒ 5ê°œ í•­ëª©ë§Œ ì¶œë ¥
    for i, article in enumerate(collected_data[:5]):
        print(f"--- í•­ëª© {i+1} ---")
        print(f"ì œëª©: {article.get('title', 'N/A')}")
        print(f"URL: {article.get('url', 'N/A')}")
        print(f"ìš”ì•½: {article.get('summary', 'N/A')[:200]}...") # ìš”ì•½ì´ ê¸¸ ìˆ˜ ìˆìœ¼ë‹ˆ ì¼ë¶€ë§Œ ì¶œë ¥
        print(f"ì¶œì²˜: {article.get('source', 'N/A')}")
        print("-" * 30)

    print(f"\nì´ {len(collected_data)}ê°œì˜ í•­ëª©ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
