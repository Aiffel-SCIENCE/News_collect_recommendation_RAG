# src/news_collector/news_collector.py
import os
import sys
import requests
import html
import json
import feedparser
from datetime import datetime, timedelta
import re
from dateutil.parser import parse as date_parse

# --- í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê³„ì‚° ë° sys.path ìˆ˜ì • ---
_current_file_path = os.path.abspath(__file__)
_project_root = os.path.dirname(os.path.dirname(os.path.dirname(_current_file_path)))
print(f"DEBUG (news_collector.py): í˜„ì¬ íŒŒì¼ ê²½ë¡œ: {_current_file_path}")
print(f"DEBUG (news_collector.py): ê³„ì‚°ëœ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {_project_root}")

if _project_root not in sys.path:
    sys.path.insert(0, _project_root)
    print(f"DEBUG (news_collector.py): sys.pathì— ì¶”ê°€ë¨: {_project_root}")
else:
    print(f"DEBUG (news_collector.py): {_project_root}ëŠ” ì´ë¯¸ sys.pathì— ìˆìŠµë‹ˆë‹¤.")

from src.pipeline_stages.initial_checks import initial_checks_task
from src.config_loader.settings import SETTINGS

# ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
DART_API_KEY = SETTINGS.get("DART_API_KEY")
NAVER_CLIENT_ID = SETTINGS.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = SETTINGS.get("NAVER_CLIENT_SECRET")
RAW_DATA_PATH = SETTINGS.get("RAW_DATA_PATH")

redis_client = None

def process_and_store_article(article_data):
    processed_article = {
        "title": article_data.get("title", ""),
        "content": "",
        "url": article_data.get("url", ""),
        "published_at": format_published_date(article_data.get("published_at")),
        "summary": article_data.get("summary", ""),
        "embedding": [],
        "source": article_data.get("source", "Unknown"),
        #"llm_info_type_categories": [],
        #"llm_topic_main_categories": [],
        #"llm_topic_sub_categories": [],
        "llm_internal_keywords": [],
        "checked": {}
    }
    save_to_raw_data_folder(processed_article)
    try:
        initial_checks_task.delay(processed_article)
        print(f"ğŸš€ Collector: '{processed_article.get('title')[:30]}...' initial_checks_taskë¡œ ì „ì†¡ë¨.")
    except Exception as e:
        print(f"âš ï¸ ê²½ê³ : Celery íƒœìŠ¤í¬ í˜¸ì¶œ ì‹¤íŒ¨ (initial_checks_task): {e}")
    return processed_article

def save_to_raw_data_folder(article):
    # Raw ë°ì´í„° JSON íŒŒì¼ ì €ì¥ì„ ë¹„í™œì„±í™”
    # raw_data_dir = os.path.join(_project_root, RAW_DATA_PATH)
    # os.makedirs(raw_data_dir, exist_ok=True)
    # filename_safe_title = re.sub(r'[\\/*?:"<>| ]', '_', article.get("title", "no_title"))[:50].strip()
    # timestamp = datetime.now().strftime("%Y%m%d%H%M%S%f")
    # filename = f"{filename_safe_title}_{timestamp}.json"
    # filepath = os.path.join(raw_data_dir, filename)
    # with open(filepath, 'w', encoding='utf-8') as f:
    #     json.dump(article, f, ensure_ascii=False, indent=4)
    # print(f"ğŸ“ raw ë°ì´í„° ì €ì¥ë¨: {filepath}")
    
    # Raw ë°ì´í„° JSON íŒŒì¼ ì €ì¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤
    pass

def format_published_date(date_str):
    if not date_str or date_str == 'No Publish Date Available':
        return None
    try:
        dt_obj = date_parse(date_str)
        return dt_obj.isoformat()
    except Exception as e:
        print(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: '{date_str}' - {e}")
        return None


def collect_from_api_file(filepath="api_sources.txt"): #
    articles = []
    sources_dir = os.path.join(_project_root, "src", "sources")
    sources_filepath = os.path.join(sources_dir, filepath)

    naver_headers = None
    if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
        naver_headers = {
            "X-Naver-Client-Id": NAVER_CLIENT_ID,
            "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
        }
        print("âœ… ë„¤ì´ë²„ API í‚¤ ë¡œë“œ ì„±ê³µ (settings.pyë¥¼ í†µí•´ config.yamlì—ì„œ)")
    else:
        print("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì—†ì–´ ë„¤ì´ë²„ API ìˆ˜ì§‘ ì‹œë„ ì‹œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. config.yaml íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    urls = []
    print(f"âœ… '{filepath}'ì—ì„œ API ì†ŒìŠ¤ URL ë¡œë“œ ì‹œë„...")
    try:
        with open(sources_filepath, "r", encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        print(f"âœ… '{filepath}'ì—ì„œ {len(urls)}ê°œì˜ API ì†ŒìŠ¤ URL ë¡œë“œ ì™„ë£Œ")
    except FileNotFoundError:
        print(f"âŒ ì—ëŸ¬: API ì†ŒìŠ¤ íŒŒì¼({sources_filepath})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return articles
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: API ì†ŒìŠ¤ íŒŒì¼({sources_filepath})ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return articles


    print("\nğŸŒ API ìˆ˜ì§‘ ì‹œì‘...")
    for url in urls:
        if not url.startswith('http'):
            print(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ URL í˜•ì‹ ê±´ë„ˆëœœ: {url}")
            continue

        data = None
        response = None
        current_headers = None
        is_naver_api = "openapi.naver.com/v1/search/news.json" in url

        if is_naver_api:
            if not naver_headers:
                print(f"âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì—†ì–´ '{url}' ìˆ˜ì§‘ ê±´ë„ˆëœ€ë‹ˆë‹¤.")
                continue
            print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ API ìˆ˜ì§‘: {url}")
            current_headers = naver_headers
        elif "thenewsapi.com/v1/news/headlines" in url or "thenewsapi.com/v1/news/top" in url:
            print(f"ğŸ” thenewsapi.com API ìˆ˜ì§‘: {url}")
        elif "gnews.io/api/v4" in url:
            print(f"ğŸ” gnews.io API ìˆ˜ì§‘: {url}")
        else:
            print(f"â“ api_sources.txtì˜ ì•Œ ìˆ˜ ì—†ëŠ” URL í˜•ì‹ ê±´ë„ˆê¹€: {url}")
            continue
        
        try:
            response = requests.get(url, headers=current_headers, timeout=10, verify=True)
            response.raise_for_status()
            data = response.json()

            items = []
            source_type = "Unknown API"

            if is_naver_api:
                items = data.get("items", [])
                source_type = "Naver News API"
            elif "thenewsapi.com" in url:
                items = data.get("data", [])
                source_type = "thenewsapi.com"
            elif "gnews.io" in url:
                items = data.get("articles", [])
                source_type = "gnews.io"

            if not items:
                print(f"âš ï¸ {source_type} ì‘ë‹µì— ì˜ˆìƒë˜ëŠ” ê¸°ì‚¬ ëª©ë¡ í‚¤ ë˜ëŠ” í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤: {url}")
            else:
                print(f"âœ… {source_type} ì‘ë‹µ í•­ëª© ìˆ˜: {len(items)}")

            count_added_from_url = 0
            for item in items:
                if isinstance(item, dict):
                    title = None
                    url_link = None
                    raw_summary = None
                    source_name = None
                    published_at = None
                    item_category = []

                    if source_type == "Naver News API":
                        title = item.get("title", "")
                        url_link = item.get("link")
                        raw_summary = item.get("description", "")
                        source_name = source_type
                        published_at = item.get('pubDate')
                    elif source_type == "thenewsapi.com":
                        title = item.get("title", "")
                        url_link = item.get("url")
                        raw_summary = item.get("snippet", "")
                        source_name = item.get("source", source_type)
                        published_at = item.get('published_at')
                    elif source_type == "gnews.io":
                        title = item.get("title", "")
                        url_link = item.get("url")
                        raw_summary = item.get("description", "")
                        source_info = item.get("source")
                        if isinstance(source_info, dict):
                            source_name = source_info.get("name", source_type)
                        else:
                            source_name = source_type
                        published_at = item.get('publishedAt')

                    cleaned_summary = html.unescape(raw_summary) if raw_summary else ""

                    if not title or not url_link:
                        print(f"âš ï¸ í•„ìˆ˜ ì •ë³´(ì œëª© ë˜ëŠ” URL) ëˆ„ë½ìœ¼ë¡œ í•­ëª© ê±´ë„ˆëœ€: {item}")
                        continue

                    article_for_pipeline = {
                        "title": title,
                        "url": url_link,
                        "summary": cleaned_summary,
                        "published_at": published_at,
                        "source": source_name,
                    }
                    if item_category:
                        article_for_pipeline["category"] = item_category

                    processed_item = process_and_store_article(article_for_pipeline)
                    articles.append(processed_item)
                    count_added_from_url += 1
                else:
                    print(f"âš ï¸ ìŠ¤í‚µë¨: API ì‘ë‹µ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœê°€ ì•„ë‹˜ â†’ {item}")
            print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {url} ({count_added_from_url}ê°œ í•­ëª© ì¶”ê°€)")

        except requests.exceptions.RequestException as e_req:
            print(f"âŒ ìš”ì²­ ì˜¤ë¥˜ ({url}): {e_req}")
        except json.JSONDecodeError as e_json:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({url}): {e_json}")
        except Exception as e_general:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ ({url}): {e_general}")


    print(f"\nâœ… API ìˆ˜ì§‘ ê³¼ì • ì™„ë£Œ. ì´ {len(articles)}ê°œ í•­ëª© ìˆ˜ì§‘ ì‹œë„.")
    return articles


def collect_from_rss_file(filepath="rss_sources.txt"): #
    articles = []
    sources_dir = os.path.join(_project_root, "src", "sources")
    sources_filepath = os.path.join(sources_dir, filepath)
    print(f"âœ… RSS ì†ŒìŠ¤ íŒŒì¼ ë¡œë“œ ì‹œë„: {sources_filepath}")

    urls = []
    try:
        with open(sources_filepath, "r", encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
        print(f"âœ… '{filepath}'ì—ì„œ {len(urls)}ê°œì˜ RSS ì†ŒìŠ¤ URL ë¡œë“œ ì™„ë£Œ")
    except FileNotFoundError:
        print(f"âŒ ì—ëŸ¬: RSS ì†ŒìŠ¤ íŒŒì¼({sources_filepath})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return articles
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: RSS ì†ŒìŠ¤ íŒŒì¼({sources_filepath})ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return articles


    print("\nğŸŒ RSS ìˆ˜ì§‘ ì‹œì‘...")
    for url in urls:
        if not url:
            continue

        print(f"\nğŸ” RSS í”¼ë“œ íŒŒì‹± ì‹œë„: {url}")
        try:
            feed = feedparser.parse(url)

            if feed.bozo:
                print(f"âš ï¸ ê²½ê³ : RSS í”¼ë“œ íŒŒì‹± ì¤‘ ë¬¸ì œ ë°œìƒ: {url} | {feed.bozo_exception}")
                if not isinstance(feed.bozo_exception, feedparser.CharacterEncodingOverride):
                    print(f"âŒ íŒŒì‹± ì˜¤ë¥˜ë¡œ í•´ë‹¹ í”¼ë“œ ê±´ë„ˆëœœ: {url}")
                    continue
            
            if not feed.entries:
                print(f"â„¹ï¸ ì •ë³´: {url} í”¼ë“œì— í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                continue

            print(f"âœ… í”¼ë“œ íŒŒì‹± ì„±ê³µ. í•­ëª© ìˆ˜: {len(feed.entries)}")

            feed_articles_count = 0
            for entry in feed.entries:
                raw_title = entry.get('title', 'No Title Available')
                raw_description = entry.get('summary', entry.get('description', 'No Description Available'))
                item_url = entry.get('link', 'No URL Available')
                pub_date = entry.get('published', entry.get('updated', 'No Publish Date Available'))

                final_title = html.unescape(raw_title).strip() if raw_title else "No Title Available"
                if raw_description:
                     final_description = html.unescape(raw_description)
                else:
                     final_description = "No Description Available"
                
                if "news.google.com/news/rss" in url:
                    final_description = "Description not available from Google News RSS"


                item_category = []
                source_name = "Unknown RSS"

                if "biopharmadive.com" in url:
                    source_name = "BioPharma Dive RSS"
                elif "fiercebiotech.com" in url:
                    source_name = "FierceBiotech RSS"
                elif "endpts.com" in url:
                    source_name = "Endpoints News RSS"
                    if 'tags' in entry and isinstance(entry.tags, list):
                        item_category = [tag.get('term') for tag in entry.tags if tag.get('term')]
                    elif 'category' in entry and entry.get('category'):
                        item_category = [entry.get('category')]
                elif "hitnews.co.kr" in url:
                    source_name = "HitNews RSS"
                elif "news.google.com/news/rss" in url:
                    source_name = "Google News RSS"
                elif "biospace.com" in url:
                    source_name = "BioSpace RSS"
                elif "pharmaphorum.com" in url:
                    source_name = "Pharmaphorum RSS"
                elif "cafepharma.com" in url:
                    source_name = "CafePharma RSS"
                elif "biopharmatrend.com" in url:
                    source_name = "BioPharma Trend RSS"
                elif "biopharmaconsortium.com" in url:
                    source_name = "BioPharm Consortium RSS"

                if item_url == 'No URL Available':
                    print(f"âš ï¸ URL ì •ë³´ ì—†ëŠ” RSS í•­ëª© ê±´ë„ˆëœ€: {final_title}")
                    continue
                
                article_for_pipeline = {
                    "title": final_title,
                    "url": item_url,
                    "summary": final_description,
                    "published_at": pub_date,
                    "source": source_name,
                }
                if item_category:
                    article_for_pipeline["category"] = item_category

                processed_item = process_and_store_article(article_for_pipeline)
                articles.append(processed_item)
                feed_articles_count += 1
            print(f"âœ… {source_name} í•­ëª© {feed_articles_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ ëˆ„ì  {len(articles)}ê°œ)")
        except Exception as e_fp:
            print(f"âŒ RSS í”¼ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({url}): {e_fp}")


    print(f"\nâœ… ì „ì²´ RSS ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(articles)}ê°œ í•­ëª© ìˆ˜ì§‘ ì‹œë„.")
    return articles


def collect_from_dart_api():
    disclosures = []
    if not DART_API_KEY:
        print("âŒ ì˜¤ë¥˜: DART_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. config.yaml íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return disclosures

    end_date = datetime.today()
    start_date = end_date - timedelta(days=89)

    url = "https://opendart.fss.or.kr/api/list.json"
    params = {
        "crtfc_key": DART_API_KEY,
        "bgn_de": start_date.strftime("%Y%m%d"),
        "end_de": end_date.strftime("%Y%m%d"),
        "page_no": 1,
        "page_count": 100
    }

    print("\nğŸŒ DART API ìˆ˜ì§‘ ì‹œì‘...")
    try:
        response = requests.get(url, params=params, timeout=10, verify=True)
        response.raise_for_status()
        data = response.json()

        if data.get("status") == "013":
            print("âŒ DART API ì˜¤ë¥˜: ì¸ì¦í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. DART_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return []
        elif data.get("status") != "000":
            print(f"âŒ DART API ì˜¤ë¥˜ ë°œìƒ: {data.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return []

        raw_list = data.get("list", [])
        print(f"âœ… DART API ì‘ë‹µ í•­ëª© ìˆ˜: {len(raw_list)}ê°œ")

        for entry in raw_list:
            disclosure_summary = f"{entry.get('rcept_dt', '')} ì ‘ìˆ˜ëœ ê³µì‹œ: {entry.get('flr_nm', 'ì œì¶œì¸ ë¶ˆëª…')}"
            article_for_pipeline = {
                "title": f"[{entry.get('corp_name', 'íšŒì‚¬ëª… ì—†ìŒ')}] {entry.get('report_nm', 'ë³´ê³ ì„œëª… ì—†ìŒ')}",
                "url": f"https://dart.fss.or.kr/dsaf001/main.do?rcpNo={entry.get('rcept_no', '')}",
                "summary": disclosure_summary,
                "published_at": entry.get("rcept_dt"),
                "source": "DART"
            }
            if not entry.get('rcept_no'):
                print(f"âš ï¸ DART í•­ëª© ê±´ë„ˆëœ€ (rcept_no ì—†ìŒ): {article_for_pipeline['title']}")
                continue

            processed_item = process_and_store_article(article_for_pipeline)
            disclosures.append(processed_item)
        print(f"âœ… DART API ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(disclosures)}ê°œ í•­ëª© ìˆ˜ì§‘ ì‹œë„.")
    except requests.exceptions.RequestException as e_req:
        print(f"âŒ DART API ìš”ì²­ ì˜¤ë¥˜: {e_req}")
    except json.JSONDecodeError as e_json:
        print(f"âŒ DART API JSON íŒŒì‹± ì˜¤ë¥˜: {e_json}")
    except Exception as e_general:
        print(f"âŒ DART API ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e_general}")

    return disclosures


def collect_all_data():
    all_collected_items = []

    print("\n--- API ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ---")
    api_data = collect_from_api_file()
    all_collected_items.extend(api_data)

    print("\n--- RSS ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ---")
    rss_data = collect_from_rss_file()
    all_collected_items.extend(rss_data)

    print("\n--- DART ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ---")
    dart_data = collect_from_dart_api()
    all_collected_items.extend(dart_data)

    print(f"\n--- ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(all_collected_items)}ê°œ í•­ëª© ì²˜ë¦¬ ì‹œë„ ---")
    return all_collected_items

if __name__ == "__main__":
    print("--- ë‰´ìŠ¤ ë° ê³µì‹œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (Celery íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì „ë‹¬) ---")
    collected_data = collect_all_data()
    print("\n--- ìˆ˜ì§‘ ì™„ë£Œ. Celery ì›Œì»¤ê°€ íƒœìŠ¤í¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. ---")
