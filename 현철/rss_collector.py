# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import feedparser
import os
import html # HTML ì—”í‹°í‹° ë””ì½”ë”©ì„ ìœ„í•´ ì„í¬íŠ¸ (íƒœê·¸ ì œê±° í›„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)
from bs4 import BeautifulSoup # HTML íƒœê·¸ ì œê±°ë¥¼ ìœ„í•´ ì„í¬íŠ¸

# RSS í”¼ë“œ íŒŒì¼ì—ì„œ URLì„ ì½ì–´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
def collect_from_rss_file(filepath="rss_sources.txt"):
    articles = [] # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    # rss_sources.txt íŒŒì¼ ê²½ë¡œë¥¼ ì´ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ì´ ì•„ë‹Œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ì—ì„œ ë‘ ë‹¨ê³„ ìœ„ë¡œ ì´ë™
    sources_filepath = os.path.join(base_dir, filepath) # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆëŠ” rss_sources.txt ê²½ë¡œ

    print(f"âœ… RSS ì†ŒìŠ¤ íŒŒì¼ ë¡œë“œ ì‹œë„: {sources_filepath}")

    # rss_sources.txt íŒŒì¼ì—ì„œ URL ëª©ë¡ ì½ê¸°
    urls = []
    try:
        with open(sources_filepath, "r", encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        print(f"âœ… '{filepath}'ì—ì„œ {len(urls)}ê°œì˜ RSS ì†ŒìŠ¤ URL ë¡œë“œ ì™„ë£Œ")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: RSS ì†ŒìŠ¤ íŒŒì¼ '{sources_filepath}' ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return articles
    except Exception as e:
        print(f"âŒ RSS ì†ŒìŠ¤ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {sources_filepath} | {e}")
        return articles

    # ë¡œë“œëœ ê° URLì„ ì²˜ë¦¬í•˜ë©° ë°ì´í„° ìˆ˜ì§‘
    for url in urls:
        print(f"ğŸ” RSS í”¼ë“œ íŒŒì‹± ì‹œë„: {url}")
        try:
            # feedparserë¥¼ ì‚¬ìš©í•˜ì—¬ RSS í”¼ë“œ íŒŒì‹±
            feed = feedparser.parse(url)

            # íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸
            if feed.bozo:
                print(f"âš ï¸ ì˜¤ë¥˜: RSS í”¼ë“œ íŒŒì‹± ì¤‘ ë¬¸ì œ ë°œìƒ: {url} | {feed.bozo_exception}")
                continue

            print(f"âœ… í”¼ë“œ íŒŒì‹± ì„±ê³µ. í•­ëª© ìˆ˜: {len(feed.entries)}")

            # --- ìƒˆë¡œ ì¶”ê°€ëœ BioPharma Dive URL ì²˜ë¦¬ ë¡œì§ ---
            if "biopharmadive.com" in url:
                 print(f"âœ¨ BioPharma Dive RSS í”¼ë“œ ({len(feed.entries)} í•­ëª©) ì²˜ë¦¬ ì¤‘...")
                 for entry in feed.entries:
                     # ì œëª©ê³¼ ì„¤ëª…ì— í¬í•¨ëœ HTML íƒœê·¸ ì œê±°
                     raw_title = entry.get('title', 'No Title Available')
                     raw_description = entry.get('summary', 'No Description Available') # ì„¤ëª…/ìš”ì•½

                     # BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                     soup_title = BeautifulSoup(raw_title, 'html.parser')
                     clean_title = soup_title.get_text()
                     final_title = html.unescape(clean_title) # HTML ì—”í‹°í‹° ë””ì½”ë”©

                     soup_desc = BeautifulSoup(raw_description, 'html.parser')
                     clean_description = soup_desc.get_text()
                     final_description = html.unescape(clean_description) # HTML ì—”í‹°í‹° ë””ì½”ë”©


                     item_data = {
                         "title": final_title,                                   # HTML íƒœê·¸ ì œê±°ëœ ì œëª©
                         "url": entry.get('link', 'No URL Available'),           # URL
                         "description": final_description,                       # HTML íƒœê·¸ ì œê±°ëœ ì„¤ëª…
                         "pubDate": entry.get('published', 'No Publish Date Available'), # ë°œí–‰ì¼
                         # categoryëŠ” ìš”ì²­ ì—†ì—ˆìœ¼ë¯€ë¡œ í¬í•¨ ì•ˆ í•¨
                         "source": "BioPharma Dive RSS",                         # ì†ŒìŠ¤ êµ¬ë¶„ ëª…ì‹œ
                     }
                     articles.append(item_data)
                 print(f"âœ… BioPharma Dive í•­ëª© {len(feed.entries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ ëˆ„ì  {len(articles)}ê°œ)")


            # --- FierceBiotech URL ì²˜ë¦¬ ë¡œì§ ---
            elif "fiercebiotech.com" in url:
                 print(f"âœ¨ FierceBiotech RSS í”¼ë“œ ({len(feed.entries)} í•­ëª©) ì²˜ë¦¬ ì¤‘...")
                 for entry in feed.entries:
                     raw_title = entry.get('title', 'No Title Available')
                     soup = BeautifulSoup(raw_title, 'html.parser')
                     clean_title = soup.get_text()
                     final_title = html.unescape(clean_title)

                     item_data = {
                         "title": final_title,
                         "url": entry.get('link', 'No URL Available'),
                         "description": entry.get('summary', 'No Description Available'),
                         "pubDate": entry.get('published', 'No Publish Date Available'),
                         "source": "FierceBiotech RSS",
                     }
                     articles.append(item_data)
                 print(f"âœ… FierceBiotech í•­ëª© {len(feed.entries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ ëˆ„ì  {len(articles)}ê°œ)")


            # --- Endpoints News URL ì²˜ë¦¬ ë¡œì§ ---
            elif "endpts.com" in url:
                print(f"âœ¨ Endpoints News RSS í”¼ë“œ ({len(feed.entries)} í•­ëª©) ì²˜ë¦¬ ì¤‘...")
                for entry in feed.entries:
                    entry_categories = []
                    if 'tags' in entry and isinstance(entry.tags, list):
                        entry_categories = [tag.get('term') for tag in entry.tags if tag.get('term')]
                    elif 'category' in entry:
                         entry_categories = [entry.get('category')] if entry.get('category') else []

                    item_data = {
                        "title": entry.get('title', 'No Title Available'),
                        "url": entry.get('link', 'No URL Available'),
                        "description": entry.get('summary', 'No Description Available'),
                        "pubDate": entry.get('published', 'No Publish Date Available'),
                        "category": entry_categories,
                        "source": "Endpoints News RSS",
                    }
                    articles.append(item_data)
                print(f"âœ… Endpoints News í•­ëª© {len(feed.entries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ ëˆ„ì  {len(articles)}ê°œ)")

            # --- HitNews URL ì²˜ë¦¬ ë¡œì§ ---
            elif "hitnews.co.kr" in url:
                print(f"âœ¨ HitNews RSS í”¼ë“œ ({len(feed.entries)} í•­ëª©) ì²˜ë¦¬ ì¤‘...")
                for entry in feed.entries:
                    item_data = {
                        "title": entry.get('title', 'No Title Available'),
                        "url": entry.get('link', 'No URL Available'),
                        "description": entry.get('summary', 'No Description Available'),
                        "pubDate": entry.get('published', 'No Publish Date Available'),
                        "source": "HitNews RSS",
                    }
                    articles.append(item_data)
                print(f"âœ… HitNews í•­ëª© {len(feed.entries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ ëˆ„ì  {len(articles)}ê°œ)")

            # --- Google News URL ì²˜ë¦¬ ë¡œì§ ---
            elif "news.google.com/news/rss" in url:
                print(f"âœ¨ Google News RSS í”¼ë“œ ({len(feed.entries)} í•­ëª©) ì²˜ë¦¬ ì¤‘...")
                for entry in feed.entries:
                    item_data = {
                        "title": entry.get('title', 'No Title Available'),
                        "url": entry.get('link', 'No URL Available'),
                        # "description":ëŠ” Google Newsì—ì„œ ì œì™¸
                        "pubDate": entry.get('published', 'No Publish Date Available'),
                        "source": "Google News RSS",
                    }
                    articles.append(item_data)
                print(f"âœ… Google News í•­ëª© {len(feed.entries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ ëˆ„ì  {len(articles)}ê°œ)")

            # --- ì•Œ ìˆ˜ ì—†ëŠ” URL ì²˜ë¦¬ ë¡œì§ ---
            else:
                print(f"â“ rss_sources.txtì˜ ì•Œ ìˆ˜ ì—†ëŠ” RSS í”¼ë“œ URL í˜•ì‹ ê±´ë„ˆê¹€: {url}")

        except Exception as e:
            print(f"âŒ RSS í”¼ë“œ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {url} | {e}")
            continue

    print(f"âœ… ì „ì²´ RSS ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(articles)}ê°œ í•­ëª© ìˆ˜ì§‘.")
    return articles
